{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c644bfb-a3da-424b-8973-c30ac2616ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import json_normalize\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "371c60c7-7128-4f71-bc12-8ee9100204f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating date variables for later use\n",
    "yesterday = datetime.now() - timedelta(1)\n",
    "yesterday = datetime.strftime(yesterday, '%Y-%m-%d')\n",
    "\n",
    "thirtydays = datetime.now() - timedelta(36)\n",
    "thirtydays = datetime.strftime(thirtydays, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "094fd77a-105d-4220-98d4-658552c12bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the API with a 400000 limit, may need to be expanded as the data set grows or may need to page if limit is exceeded\n",
    "api_url = \"https://healthdata.gov/resource/j8mb-icvb.json?$limit=400000\"\n",
    "r = requests.get(api_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ea1dfe7-1c94-4174-84b9-b37df3ab4ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       state state_name state_fips fema_region overall_outcome  \\\n",
      "0         AL    Alabama         01    Region 4        Negative   \n",
      "1         AL    Alabama         01    Region 4        Positive   \n",
      "2         AL    Alabama         01    Region 4        Negative   \n",
      "3         AL    Alabama         01    Region 4        Positive   \n",
      "4         AL    Alabama         01    Region 4        Negative   \n",
      "...      ...        ...        ...         ...             ...   \n",
      "188434    WY    Wyoming         56    Region 8        Negative   \n",
      "188435    WY    Wyoming         56    Region 8        Positive   \n",
      "188436    WY    Wyoming         56    Region 8    Inconclusive   \n",
      "188437    WY    Wyoming         56    Region 8        Negative   \n",
      "188438    WY    Wyoming         56    Region 8        Positive   \n",
      "\n",
      "                           date new_results_reported total_results_reported  \n",
      "0       2020-03-01T00:00:00.000                   96                     96  \n",
      "1       2020-03-01T00:00:00.000                   16                     16  \n",
      "2       2020-03-02T00:00:00.000                   72                    168  \n",
      "3       2020-03-02T00:00:00.000                    6                     22  \n",
      "4       2020-03-03T00:00:00.000                   94                    262  \n",
      "...                         ...                  ...                    ...  \n",
      "188434  2023-04-30T00:00:00.000                   63                1437196  \n",
      "188435  2023-04-30T00:00:00.000                    7                 137897  \n",
      "188436  2023-05-01T00:00:00.000                    0                   4013  \n",
      "188437  2023-05-01T00:00:00.000                   27                1437223  \n",
      "188438  2023-05-01T00:00:00.000                    2                 137899  \n",
      "\n",
      "[188439 rows x 8 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "694336881"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taking the called json data, converting to a dataframe\n",
    "json = r.json()\n",
    "df = pd.DataFrame(json)\n",
    "print(df)\n",
    "#Converting datatypes to necessary forms\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.astype({'new_results_reported':'int'})\n",
    "df = df.astype({'total_results_reported':'int'})\n",
    "#Question 1 Result\n",
    "#df.loc[df['date'] == yesterday ,'total_results_reported'].sum()\n",
    "df.loc[df['date'] == df['date'].max() ,'total_results_reported'].sum()\n",
    "#So theres a few things here that were interesting, the total is a rolling number so you only need to call the latest one to get the desired result. \n",
    "#Additionally this API appears to update every monday potentially? So getting down to yesterday reliably does not seem like its often possible. \n",
    "#Because of this I included two commands, one to call yesterday if that data is in the dataframe, and if not the more applicable one which is just getting\n",
    "#the sum of the total results reported on the latest day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eee726a-e60a-4250-af44-aa12ac4271bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cloon\\AppData\\Local\\Temp\\ipykernel_20332\\3721394359.py:9: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  freq='1D', sort=True)).sum()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_results_reported</th>\n",
       "      <th>rolling_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-04-02</th>\n",
       "      <td>8693</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-03</th>\n",
       "      <td>13879</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-04</th>\n",
       "      <td>15471</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-05</th>\n",
       "      <td>13658</td>\n",
       "      <td>12898.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-06</th>\n",
       "      <td>12103</td>\n",
       "      <td>12521.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-07</th>\n",
       "      <td>11124</td>\n",
       "      <td>12179.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-08</th>\n",
       "      <td>8791</td>\n",
       "      <td>11959.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-09</th>\n",
       "      <td>6981</td>\n",
       "      <td>11715.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-10</th>\n",
       "      <td>12279</td>\n",
       "      <td>11486.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-11</th>\n",
       "      <td>13397</td>\n",
       "      <td>11190.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-12</th>\n",
       "      <td>12853</td>\n",
       "      <td>11075.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-13</th>\n",
       "      <td>11764</td>\n",
       "      <td>11027.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-14</th>\n",
       "      <td>10242</td>\n",
       "      <td>10901.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-15</th>\n",
       "      <td>7740</td>\n",
       "      <td>10750.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-16</th>\n",
       "      <td>6321</td>\n",
       "      <td>10656.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-17</th>\n",
       "      <td>10395</td>\n",
       "      <td>10387.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-18</th>\n",
       "      <td>10531</td>\n",
       "      <td>9978.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-19</th>\n",
       "      <td>10426</td>\n",
       "      <td>9631.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-20</th>\n",
       "      <td>9373</td>\n",
       "      <td>9289.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-21</th>\n",
       "      <td>8663</td>\n",
       "      <td>9064.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-22</th>\n",
       "      <td>6278</td>\n",
       "      <td>8855.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-23</th>\n",
       "      <td>5306</td>\n",
       "      <td>8710.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-24</th>\n",
       "      <td>8044</td>\n",
       "      <td>8374.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-25</th>\n",
       "      <td>7892</td>\n",
       "      <td>7997.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-26</th>\n",
       "      <td>7929</td>\n",
       "      <td>7640.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-27</th>\n",
       "      <td>7212</td>\n",
       "      <td>7332.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-28</th>\n",
       "      <td>6272</td>\n",
       "      <td>6990.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-29</th>\n",
       "      <td>4822</td>\n",
       "      <td>6782.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-30</th>\n",
       "      <td>3587</td>\n",
       "      <td>6536.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-01</th>\n",
       "      <td>2492</td>\n",
       "      <td>5743.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            new_results_reported  rolling_mean\n",
       "date                                          \n",
       "2023-04-02                  8693           NaN\n",
       "2023-04-03                 13879           NaN\n",
       "2023-04-04                 15471           NaN\n",
       "2023-04-05                 13658  12898.571429\n",
       "2023-04-06                 12103  12521.714286\n",
       "2023-04-07                 11124  12179.571429\n",
       "2023-04-08                  8791  11959.857143\n",
       "2023-04-09                  6981  11715.285714\n",
       "2023-04-10                 12279  11486.714286\n",
       "2023-04-11                 13397  11190.428571\n",
       "2023-04-12                 12853  11075.428571\n",
       "2023-04-13                 11764  11027.000000\n",
       "2023-04-14                 10242  10901.000000\n",
       "2023-04-15                  7740  10750.857143\n",
       "2023-04-16                  6321  10656.571429\n",
       "2023-04-17                 10395  10387.428571\n",
       "2023-04-18                 10531   9978.000000\n",
       "2023-04-19                 10426   9631.285714\n",
       "2023-04-20                  9373   9289.714286\n",
       "2023-04-21                  8663   9064.142857\n",
       "2023-04-22                  6278   8855.285714\n",
       "2023-04-23                  5306   8710.285714\n",
       "2023-04-24                  8044   8374.428571\n",
       "2023-04-25                  7892   7997.428571\n",
       "2023-04-26                  7929   7640.714286\n",
       "2023-04-27                  7212   7332.000000\n",
       "2023-04-28                  6272   6990.428571\n",
       "2023-04-29                  4822   6782.428571\n",
       "2023-04-30                  3587   6536.857143\n",
       "2023-05-01                  2492   5743.714286"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Question Two process\n",
    "#Filtering down to only positive results over last 30 days and grouping by date\n",
    "df2_positive_trunc=df.loc[(df['overall_outcome'] == \"Positive\") & (df[\"date\"] >= thirtydays)]\n",
    "df2_positive_agg=df2_positive_trunc[['date','new_results_reported']].groupby(['date']).sum()\n",
    "#Calculating the rolling mean and results\n",
    "df2_positive_agg['rolling_mean'] = df2_positive_agg['new_results_reported'].rolling(window=7).mean()\n",
    "display(df2_positive_agg.tail(30))\n",
    "#A few things here as well, the offset 4 days of missing data does make this a little weird, I wasn't sure whether it was more correct to go 30 days back\n",
    "#in the available data or 30 days from today and be missing a few values. I went with the latter I assume both approaches could be fine depending on \n",
    "#what people are looking for. This one is a little tricky because there are inconclusive results that I assume get updated later or retested and sent in as\n",
    "#positive or negative for some states. So while this mean does get a snapshot of something close its worth being aware this may not be the true number of \n",
    "#positive tests since I don't know how each state handles unsure results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "63ef886f-980e-4b6b-98ca-b434f9597308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positivity_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Iowa</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U.S. Virgin Islands</th>\n",
       "      <td>21.839080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missouri</th>\n",
       "      <td>20.530434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Dakota</th>\n",
       "      <td>18.234203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oklahoma</th>\n",
       "      <td>12.918975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Puerto Rico</th>\n",
       "      <td>12.671575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hawaii</th>\n",
       "      <td>11.772240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Mexico</th>\n",
       "      <td>11.537414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>11.122692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mississippi</th>\n",
       "      <td>10.383545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     positivity_rate\n",
       "state_name                          \n",
       "Iowa                      100.000000\n",
       "U.S. Virgin Islands        21.839080\n",
       "Missouri                   20.530434\n",
       "South Dakota               18.234203\n",
       "Oklahoma                   12.918975\n",
       "Puerto Rico                12.671575\n",
       "Hawaii                     11.772240\n",
       "New Mexico                 11.537414\n",
       "Nevada                     11.122692\n",
       "Mississippi                10.383545"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Question Three process\n",
    "#Same idea of truncating down to the necessary range for two seperate data sets\n",
    "df3_datetrunc=df.loc[(df[\"date\"] >= thirtydays)]\n",
    "df3_positive_datetrunc=df.loc[(df['overall_outcome'] == \"Positive\") & (df[\"date\"] >= thirtydays)]\n",
    "#Grouping both of those data sets to results reported\n",
    "df3_dateagg=df3_datetrunc[['state_name','new_results_reported']].groupby(['state_name']).sum()\n",
    "df3_positive_dateagg=df3_positive_datetrunc[['state_name','new_results_reported']].groupby(['state_name']).sum()\n",
    "#Merging the data & calculating the percentage\n",
    "merged_df3 = pd.merge(df3_dateagg, df3_positive_dateagg, how='outer', on = ['state_name'])\n",
    "merged_df3_posrate = mergeddf3.assign(positivity_rate=lambda x: x.new_results_reported_y / x.new_results_reported_x * 100)\n",
    "#Cleaning data and filtering to the necessary range\n",
    "merged_df3_posrate = merged_df3_posrate.drop(columns=['new_results_reported_x','new_results_reported_y'])\n",
    "merged_df3_clean=merged_df3_posrate.sort_values(by = 'positivity_rate', ascending = False).head(10)\n",
    "display(merged_df3_clean)\n",
    "\n",
    "#So this approach is a bit convoluted, I split the data sets and merged them back to get the two fields needed for calculating. I am sure thats not \n",
    "#the absolute ideal way compared to having some redundant values with the positives and trimming back from there. For data the size of the truncated \n",
    "#things we are working with I think the approach is fine but if it scaled to larger or more complex data a solution that does not require loading the \n",
    "#dataset twice and joining it on itself is obviously preferred. This was just what made sense in my head going step by step through the cleaning process. \n",
    "#As for insights it looks like Iowa only reports positive results meaning they seem to be much higher but would surely be more in line with the rest if \n",
    "#they reported everything. Additionally those inconclusive results might not be used by every state, and depending on where they would get bucketed by \n",
    "#those states that could swing their positivity rate somewhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5173f7b1-6b36-4b22-9213-7fcca947f334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
